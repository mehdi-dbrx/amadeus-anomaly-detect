{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811cd527-c1d7-4464-9e1c-3b6a8a865f68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Load data and select columns"
    }
   },
   "outputs": [],
   "source": [
    "# Load the table and select only required columns\n",
    "df = spark.table(\"mc.amadeus2.data_jan26\").select(\n",
    "    \"trip_origin_city\",\n",
    "    \"trip_destination_city\",\n",
    "    \"flight_leg_departure_date\",\n",
    "    \"flight_leg_total_seats\"\n",
    ")\n",
    "\n",
    "# Display sample data\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412a15a0-d628-48b4-a997-02b0ef0e4dd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Create route feature"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, dayofweek\n",
    "\n",
    "# Create route feature: origin_to_destination\n",
    "df_with_route = df.withColumn(\n",
    "    \"route\",\n",
    "    concat(\"trip_origin_city\", lit(\"_to_\"), \"trip_destination_city\")\n",
    ")\n",
    "\n",
    "# Extract day_of_week for grouping (1=Sunday, 7=Saturday)\n",
    "df_with_route = df_with_route.withColumn(\n",
    "    \"day_of_week\",\n",
    "    dayofweek(\"flight_leg_departure_date\")\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_with_route.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095790e6-03a3-49b7-acc8-337588e719cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Group by route and day_of_week, aggregate mean seats"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "# Group by route and day_of_week, calculate mean of flight_leg_total_seats\n",
    "df_aggregated = df_with_route.groupBy(\"route\", \"day_of_week\").agg(\n",
    "    round(avg(\"flight_leg_total_seats\"), 0).alias(\"avg_seats\")\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_aggregated.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9d1aae-b02d-424c-9568-d6ad5966e38f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: Convert to pandas and apply IsolationForest"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_pandas = df_aggregated.toPandas()\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "# Fit on avg_seats only (reshape for sklearn)\n",
    "X = df_pandas[['avg_seats']]\n",
    "iso_forest.fit(X)\n",
    "\n",
    "print(f\"Model trained on {len(df_pandas)} route-day combinations\")\n",
    "print(f\"avg_seats range: {df_pandas['avg_seats'].min()} - {df_pandas['avg_seats'].max()}\")\n",
    "display(df_pandas.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9b89f5-2dbe-4109-8dac-8794b3a3c187",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 6: Predict anomalies (-1 = anomaly, 1 = normal)"
    }
   },
   "outputs": [],
   "source": [
    "# Predict anomalies: -1 = anomaly, 1 = normal\n",
    "df_pandas['anomaly'] = iso_forest.predict(X)\n",
    "\n",
    "# Count anomalies\n",
    "anomalies_count = (df_pandas['anomaly'] == -1).sum()\n",
    "normal_count = (df_pandas['anomaly'] == 1).sum()\n",
    "\n",
    "print(f\"Total records: {len(df_pandas)}\")\n",
    "print(f\"Anomalies detected: {anomalies_count} ({anomalies_count/len(df_pandas)*100:.2f}%)\")\n",
    "print(f\"Normal records: {normal_count} ({normal_count/len(df_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Display sample with anomalies\n",
    "print(\"\\nSample of detected anomalies:\")\n",
    "display(df_pandas[df_pandas['anomaly'] == -1].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b30c99-042f-42b1-b90a-db76ee063a31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 7: Log model with MLflow"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"flight_seat_anomaly_detection\") as run:\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"IsolationForest\")\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"feature\", \"avg_seats\")\n",
    "    mlflow.log_param(\"total_records\", len(df_pandas))\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"anomalies_count\", anomalies_count)\n",
    "    mlflow.log_metric(\"anomalies_percentage\", anomalies_count/len(df_pandas)*100)\n",
    "    mlflow.log_metric(\"normal_count\", normal_count)\n",
    "    mlflow.log_metric(\"avg_seats_min\", df_pandas['avg_seats'].min())\n",
    "    mlflow.log_metric(\"avg_seats_max\", df_pandas['avg_seats'].max())\n",
    "    \n",
    "    # Create model signature\n",
    "    predictions = iso_forest.predict(X)\n",
    "    signature = infer_signature(X, predictions)\n",
    "    \n",
    "    # Log the model with signature\n",
    "    mlflow.sklearn.log_model(iso_forest, \"isolation_forest_model\", signature=signature)\n",
    "    \n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "    print(f\"Model logged successfully with signature!\")\n",
    "    print(f\"Experiment ID: {run.info.experiment_id}\")\n",
    "    print(f\"Artifact URI: {run.info.artifact_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579b72fa-3aa3-4f0f-92cc-32dc9d7760b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 8: Register model to Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "# Register the model to Unity Catalog\n",
    "model_name = \"mc.amadeus2.flight_seat_anomaly_detector\"\n",
    "\n",
    "# Get the model URI from the last run\n",
    "model_uri = f\"runs:/{run.info.run_id}/isolation_forest_model\"\n",
    "\n",
    "# Register the model\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model registered successfully!\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Model version: {registered_model.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464fbb1a-fdcf-4a4a-a22a-e9541709ed0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploy to Model Serving"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ServedEntityInput, EndpointCoreConfigInput\n",
    "\n",
    "# Initialize Databricks client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Define endpoint configuration\n",
    "endpoint_name = \"flight-seat-anomaly-detector\"\n",
    "model_name = \"mc.amadeus2.flight_seat_anomaly_detector\"\n",
    "model_version = registered_model.version\n",
    "\n",
    "# Create or update the serving endpoint\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=EndpointCoreConfigInput(\n",
    "            served_entities=[\n",
    "                ServedEntityInput(\n",
    "                    entity_name=model_name,\n",
    "                    entity_version=str(model_version),\n",
    "                    scale_to_zero_enabled=True,\n",
    "                    workload_size=\"Small\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    print(f\"Model serving endpoint created successfully!\")\n",
    "    print(f\"Endpoint name: {endpoint_name}\")\n",
    "    print(f\"Endpoint state: {endpoint.state.config_update}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"Endpoint '{endpoint_name}' already exists. Updating...\")\n",
    "        endpoint = w.serving_endpoints.update_config_and_wait(\n",
    "            name=endpoint_name,\n",
    "            served_entities=[\n",
    "                ServedEntityInput(\n",
    "                    entity_name=model_name,\n",
    "                    entity_version=str(model_version),\n",
    "                    scale_to_zero_enabled=True,\n",
    "                    workload_size=\"Small\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Endpoint updated successfully!\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99a90b4-d5f5-4930-b4e0-79bb3e4a6802",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load and prepare anomaly_updates data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, dayofweek, avg, round\n",
    "\n",
    "# Load the anomaly_updates table\n",
    "df_updates = spark.table(\"mc.amadeus2.anomaly_updates\").select(\n",
    "    \"trip_origin_city\",\n",
    "    \"trip_destination_city\",\n",
    "    \"flight_leg_departure_date\",\n",
    "    \"new_flight_leg_total_seats\"\n",
    ")\n",
    "\n",
    "# Create route feature: origin_to_destination\n",
    "df_updates_with_route = df_updates.withColumn(\n",
    "    \"route\",\n",
    "    concat(\"trip_origin_city\", lit(\"_to_\"), \"trip_destination_city\")\n",
    ")\n",
    "\n",
    "# Extract day_of_week for grouping\n",
    "df_updates_with_route = df_updates_with_route.withColumn(\n",
    "    \"day_of_week\",\n",
    "    dayofweek(\"flight_leg_departure_date\")\n",
    ")\n",
    "\n",
    "# Group by route and day_of_week, calculate mean of new_flight_leg_total_seats\n",
    "df_updates_aggregated = df_updates_with_route.groupBy(\"route\", \"day_of_week\").agg(\n",
    "    round(avg(\"new_flight_leg_total_seats\"), 0).alias(\"avg_seats\")\n",
    ")\n",
    "\n",
    "print(f\"Total route-day combinations to score: {df_updates_aggregated.count()}\")\n",
    "display(df_updates_aggregated.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "957cc258-dd68-4fe2-afad-6accfc912cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_updates_aggregated.write.mode(\"overwrite\").saveAsTable(\"mc.amadeus2.anomaly_updates_aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6519f3-63e0-4447-9903-88fa8d040407",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call deployed model endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get the endpoint URL and token\n",
    "endpoint_name = \"flight-seat-anomaly-detector\"\n",
    "token = w.dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "host = w.dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "\n",
    "# Prepare data for inference - convert to pandas and format as required by the model\n",
    "df_updates_pandas = df_updates_aggregated.toPandas()\n",
    "\n",
    "# Handle NaN values - drop rows with NaN in avg_seats\n",
    "print(f\"Total records before cleaning: {len(df_updates_pandas)}\")\n",
    "df_updates_pandas = df_updates_pandas.dropna(subset=['avg_seats'])\n",
    "print(f\"Total records after removing NaN: {len(df_updates_pandas)}\")\n",
    "\n",
    "# The model expects a dataframe with 'avg_seats' column\n",
    "input_data = df_updates_pandas[['avg_seats']].to_dict(orient='split')\n",
    "\n",
    "# Call the endpoint\n",
    "url = f\"{host}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"dataframe_split\": input_data\n",
    "}\n",
    "\n",
    "print(f\"\\nCalling endpoint: {endpoint_name}\")\n",
    "print(f\"Scoring {len(df_updates_pandas)} records...\")\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    predictions = response.json()['predictions']\n",
    "    df_updates_pandas['anomaly'] = predictions\n",
    "    print(f\"\\n‚úì Successfully scored {len(predictions)} records\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    raise Exception(f\"Model endpoint call failed with status {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e47cfe-9843-4c3f-a40a-fac4be67ef3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyze and display anomaly results"
    }
   },
   "outputs": [],
   "source": [
    "# Count anomalies\n",
    "anomalies_count = (df_updates_pandas['anomaly'] == -1).sum()\n",
    "normal_count = (df_updates_pandas['anomaly'] == 1).sum()\n",
    "\n",
    "print(f\"\\n=== ANOMALY DETECTION RESULTS ===\")\n",
    "print(f\"Total records scored: {len(df_updates_pandas)}\")\n",
    "print(f\"Anomalies detected: {anomalies_count} ({anomalies_count/len(df_updates_pandas)*100:.2f}%)\")\n",
    "print(f\"Normal records: {normal_count} ({normal_count/len(df_updates_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Display anomalies sorted by avg_seats (highest first)\n",
    "print(\"\\n=== DETECTED ANOMALIES (sorted by avg_seats) ===\")\n",
    "anomalies_df = df_updates_pandas[df_updates_pandas['anomaly'] == -1].sort_values('avg_seats', ascending=False)\n",
    "display(anomalies_df)\n",
    "\n",
    "# Display normal records sample\n",
    "print(\"\\n=== SAMPLE OF NORMAL RECORDS ===\")\n",
    "normal_df = df_updates_pandas[df_updates_pandas['anomaly'] == 1].sort_values('avg_seats', ascending=False)\n",
    "display(normal_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71d18b4-8a40-4c09-bb4a-c9f5cb700914",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate anomaly scores and % deviation from normal"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load the model from Unity Catalog to get anomaly scores (use version 1)\n",
    "model_name = \"mc.amadeus2.flight_seat_anomaly_detector\"\n",
    "loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/1\")\n",
    "\n",
    "# Get anomaly scores from the model (more negative = more anomalous)\n",
    "anomalies_scores = loaded_model.score_samples(df_updates_pandas[['avg_seats']])\n",
    "df_updates_pandas['anomaly_score'] = anomalies_scores\n",
    "\n",
    "# Calculate baseline from normal records\n",
    "normal_baseline = df_updates_pandas[df_updates_pandas['anomaly'] == 1]['avg_seats'].median()\n",
    "normal_mean = df_updates_pandas[df_updates_pandas['anomaly'] == 1]['avg_seats'].mean()\n",
    "normal_std = df_updates_pandas[df_updates_pandas['anomaly'] == 1]['avg_seats'].std()\n",
    "\n",
    "# Calculate deviation multiplier (divide by 100, round to 0, add +/- sign)\n",
    "deviation_raw = ((df_updates_pandas['avg_seats'] - normal_baseline) / normal_baseline).round(0)\n",
    "df_updates_pandas['deviation_multiplier'] = deviation_raw.apply(lambda x: f\"+{int(x)}%\" if x > 0 else f\"{int(x)}x\")\n",
    "\n",
    "# Calculate standard deviations from mean\n",
    "df_updates_pandas['std_deviations'] = ((df_updates_pandas['avg_seats'] - normal_mean) / normal_std).round(2)\n",
    "\n",
    "print(f\"=== NORMAL BASELINE STATISTICS ===\")\n",
    "print(f\"Median (baseline): {normal_baseline:.0f} seats\")\n",
    "print(f\"Mean: {normal_mean:.2f} seats\")\n",
    "print(f\"Std Dev: {normal_std:.2f} seats\")\n",
    "print(f\"\\n=== TOP 20 ANOMALIES WITH DEVIATION METRICS ===\")\n",
    "\n",
    "# Show top anomalies with all metrics (hide anomaly_score)\n",
    "top_anomalies = df_updates_pandas[df_updates_pandas['anomaly'] == -1].sort_values('avg_seats', ascending=False).head(20)\n",
    "display(top_anomalies[['route', 'day_of_week', 'avg_seats', 'deviation_multiplier', 'std_deviations']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5a26de-f9da-4096-8813-1fde7edd349f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enrich anomalies with IATA city/country names"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Convert anomalies to Spark DataFrame\n",
    "df_anomalies_spark = spark.createDataFrame(df_updates_pandas[df_updates_pandas['anomaly'] == -1])\n",
    "\n",
    "# Split route to get origin and destination IATA codes\n",
    "df_anomalies_split = df_anomalies_spark.withColumn(\n",
    "    \"origin_city\", split(\"route\", \"_to_\")[0]\n",
    ").withColumn(\n",
    "    \"destination_city\", split(\"route\", \"_to_\")[1]\n",
    ")\n",
    "\n",
    "# Load IATA lookup table\n",
    "df_iata = spark.table(\"mc.amadeus2.iata\")\n",
    "\n",
    "# Join for origin city/country\n",
    "df_enriched = df_anomalies_split.alias(\"a\").join(\n",
    "    df_iata.alias(\"i_origin\"),\n",
    "    col(\"a.origin_city\") == col(\"i_origin.iata\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"a.*\"),\n",
    "    col(\"i_origin.city\").alias(\"origin_city_full\"),\n",
    "    col(\"i_origin.country\").alias(\"origin_country_full\")\n",
    ")\n",
    "\n",
    "# Join for destination city/country\n",
    "df_enriched = df_enriched.alias(\"a\").join(\n",
    "    df_iata.alias(\"i_dest\"),\n",
    "    col(\"a.destination_city\") == col(\"i_dest.iata\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"a.route\"),\n",
    "    col(\"a.origin_city\"),\n",
    "    col(\"a.origin_city_full\"),\n",
    "    col(\"a.origin_country_full\"),\n",
    "    col(\"a.destination_city\"),\n",
    "    col(\"i_dest.city\").alias(\"destination_city_full\"),\n",
    "    col(\"i_dest.country\").alias(\"destination_country_full\"),\n",
    "    col(\"a.day_of_week\"),\n",
    "    col(\"a.avg_seats\"),\n",
    "    col(\"a.deviation_multiplier\"),\n",
    "    col(\"a.std_deviations\")\n",
    ")\n",
    "\n",
    "print(f\"‚úì Enriched {df_enriched.count()} anomalies with city/country names\")\n",
    "print(\"\\n=== Top 20 Enriched Anomalies ===\")\n",
    "display(df_enriched.orderBy(\"avg_seats\", ascending=False).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e3193d-0948-4bf4-b3ed-8b696502f784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Flight Seat Anomaly Detection - Model Inference Instructions\n",
    "\n",
    "## Overview\n",
    "This document provides instructions for autonomously performing anomaly detection inference on flight seat data using the deployed model endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "### Input Table\n",
    "- **Full Path**: `mc.amadeus2.anomaly_updates`\n",
    "- **Schema**: \n",
    "  - `trip_origin_city` (STRING)\n",
    "  - `trip_destination_city` (STRING)\n",
    "  - `flight_leg_departure_date` (DATE)\n",
    "  - `flight_leg_origin_city` (STRING)\n",
    "  - `flight_leg_destination_city` (STRING)\n",
    "  - `new_flight_leg_total_seats` (INT)\n",
    "  - `flight_leg_total_seats` (INT)\n",
    "  - `multiplier` (DOUBLE)\n",
    "\n",
    "### Lookup Table\n",
    "- **Full Path**: `mc.amadeus2.iata`\n",
    "- **Schema**:\n",
    "  - `city` (STRING) - Full city name\n",
    "  - `country` (STRING) - Full country name\n",
    "  - `iata` (STRING) - IATA airport code\n",
    "\n",
    "### Model Endpoint\n",
    "- **Endpoint Name**: `flight-seat-anomaly-detector`\n",
    "- **Model Name**: `mc.amadeus2.flight_seat_anomaly_detector`\n",
    "- **Model Version**: 1\n",
    "- **Input Feature**: `avg_seats` (single numeric column)\n",
    "- **Output**: Anomaly prediction (-1 = anomaly, 1 = normal)\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Functions\n",
    "\n",
    "### Function 1: Load and Prepare Data\n",
    "```python\n",
    "def load_and_prepare_anomaly_data(table_name: str = \"mc.amadeus2.anomaly_updates\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load anomaly updates table and prepare for model inference.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Full path to the anomaly updates table\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with route, day_of_week, and avg_seats columns\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import concat, lit, dayofweek, avg, round\n",
    "    \n",
    "    # Load table\n",
    "    df = spark.table(table_name).select(\n",
    "        \"trip_origin_city\",\n",
    "        \"trip_destination_city\",\n",
    "        \"flight_leg_departure_date\",\n",
    "        \"new_flight_leg_total_seats\"\n",
    "    )\n",
    "    \n",
    "    # Create route feature\n",
    "    df = df.withColumn(\n",
    "        \"route\",\n",
    "        concat(\"trip_origin_city\", lit(\"_to_\"), \"trip_destination_city\")\n",
    "    )\n",
    "    \n",
    "    # Extract day of week\n",
    "    df = df.withColumn(\n",
    "        \"day_of_week\",\n",
    "        dayofweek(\"flight_leg_departure_date\")\n",
    "    )\n",
    "    \n",
    "    # Aggregate by route and day_of_week\n",
    "    df_agg = df.groupBy(\"route\", \"day_of_week\").agg(\n",
    "        round(avg(\"new_flight_leg_total_seats\"), 0).alias(\"avg_seats\")\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas and clean\n",
    "    df_pandas = df_agg.toPandas()\n",
    "    df_pandas = df_pandas.dropna(subset=['avg_seats'])\n",
    "    \n",
    "    print(f\"‚úì Loaded and prepared {len(df_pandas)} route-day combinations\")\n",
    "    return df_pandas\n",
    "```\n",
    "\n",
    "### Function 2: Invoke Model Endpoint\n",
    "```python\n",
    "def invoke_anomaly_model(df: pd.DataFrame, endpoint_name: str = \"flight-seat-anomaly-detector\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Call the deployed model endpoint to get anomaly predictions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'avg_seats' column\n",
    "        endpoint_name: Name of the serving endpoint\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'anomaly' column (-1 or 1)\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    \n",
    "    # Initialize client\n",
    "    w = WorkspaceClient()\n",
    "    token = w.dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    host = w.dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    \n",
    "    # Prepare payload\n",
    "    input_data = df[['avg_seats']].to_dict(orient='split')\n",
    "    payload = {\"dataframe_split\": input_data}\n",
    "    \n",
    "    # Call endpoint\n",
    "    url = f\"{host}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        predictions = response.json()['predictions']\n",
    "        df['anomaly'] = predictions\n",
    "        print(f\"‚úì Successfully scored {len(predictions)} records\")\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"Model endpoint call failed: {response.status_code} - {response.text}\")\n",
    "```\n",
    "\n",
    "### Function 3: Calculate Deviation Metrics\n",
    "```python\n",
    "def calculate_deviation_metrics(df: pd.DataFrame, model_name: str = \"mc.amadeus2.flight_seat_anomaly_detector\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate anomaly scores and deviation metrics from normal baseline.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'avg_seats' and 'anomaly' columns\n",
    "        model_name: Full model name in Unity Catalog\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added deviation metrics columns\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    \n",
    "    # Load model to get anomaly scores\n",
    "    loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/1\")\n",
    "    anomaly_scores = loaded_model.score_samples(df[['avg_seats']])\n",
    "    df['anomaly_score'] = anomaly_scores\n",
    "    \n",
    "    # Calculate baseline from normal records\n",
    "    normal_baseline = df[df['anomaly'] == 1]['avg_seats'].median()\n",
    "    normal_mean = df[df['anomaly'] == 1]['avg_seats'].mean()\n",
    "    normal_std = df[df['anomaly'] == 1]['avg_seats'].std()\n",
    "    \n",
    "    # Calculate deviation multiplier\n",
    "    deviation_raw = ((df['avg_seats'] - normal_baseline) / normal_baseline).round(0)\n",
    "    df['deviation_multiplier'] = deviation_raw.apply(lambda x: f\"+{int(x)}x\" if x > 0 else f\"{int(x)}x\")\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    df['std_deviations'] = ((df['avg_seats'] - normal_mean) / normal_std).round(2)\n",
    "    \n",
    "    print(f\"‚úì Calculated deviation metrics\")\n",
    "    print(f\"  Normal baseline (median): {normal_baseline:.0f} seats\")\n",
    "    print(f\"  Normal mean: {normal_mean:.2f} seats\")\n",
    "    print(f\"  Normal std dev: {normal_std:.2f} seats\")\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "### Function 4: Enrich with IATA Data\n",
    "```python\n",
    "def enrich_with_iata_data(df: pd.DataFrame, iata_table: str = \"mc.amadeus2.iata\"):\n",
    "    \"\"\"\n",
    "    Enrich anomaly data with full city and country names from IATA lookup.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'route' column and anomaly flag\n",
    "        iata_table: Full path to IATA lookup table\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame with enriched city/country information\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import split, col\n",
    "    \n",
    "    # Filter anomalies only\n",
    "    df_anomalies = df[df['anomaly'] == -1]\n",
    "    \n",
    "    # Convert to Spark\n",
    "    df_spark = spark.createDataFrame(df_anomalies)\n",
    "    \n",
    "    # Split route into origin and destination\n",
    "    df_split = df_spark.withColumn(\n",
    "        \"origin_city\", split(\"route\", \"_to_\")[0]\n",
    "    ).withColumn(\n",
    "        \"destination_city\", split(\"route\", \"_to_\")[1]\n",
    "    )\n",
    "    \n",
    "    # Load IATA table\n",
    "    df_iata = spark.table(iata_table)\n",
    "    \n",
    "    # Join for origin\n",
    "    df_enriched = df_split.alias(\"a\").join(\n",
    "        df_iata.alias(\"i_origin\"),\n",
    "        col(\"a.origin_city\") == col(\"i_origin.iata\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        col(\"a.*\"),\n",
    "        col(\"i_origin.city\").alias(\"origin_city_full\"),\n",
    "        col(\"i_origin.country\").alias(\"origin_country_full\")\n",
    "    )\n",
    "    \n",
    "    # Join for destination\n",
    "    df_enriched = df_enriched.alias(\"a\").join(\n",
    "        df_iata.alias(\"i_dest\"),\n",
    "        col(\"a.destination_city\") == col(\"i_dest.iata\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        col(\"a.route\"),\n",
    "        col(\"a.origin_city\"),\n",
    "        col(\"a.origin_city_full\"),\n",
    "        col(\"a.origin_country_full\"),\n",
    "        col(\"a.destination_city\"),\n",
    "        col(\"i_dest.city\").alias(\"destination_city_full\"),\n",
    "        col(\"i_dest.country\").alias(\"destination_country_full\"),\n",
    "        col(\"a.day_of_week\"),\n",
    "        col(\"a.avg_seats\"),\n",
    "        col(\"a.deviation_multiplier\"),\n",
    "        col(\"a.std_deviations\")\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Enriched {df_enriched.count()} anomalies with IATA data\")\n",
    "    return df_enriched\n",
    "```\n",
    "\n",
    "### Function 5: Display Results\n",
    "```python\n",
    "def display_anomaly_results(df: pd.DataFrame, df_enriched):\n",
    "    \"\"\"\n",
    "    Display comprehensive anomaly detection results.\n",
    "    \n",
    "    Args:\n",
    "        df: Full DataFrame with all predictions and metrics\n",
    "        df_enriched: Spark DataFrame with enriched anomaly data\n",
    "    \"\"\"\n",
    "    # Summary statistics\n",
    "    total = len(df)\n",
    "    anomalies = (df['anomaly'] == -1).sum()\n",
    "    normal = (df['anomaly'] == 1).sum()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANOMALY DETECTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total records scored: {total:,}\")\n",
    "    print(f\"Anomalies detected: {anomalies:,} ({anomalies/total*100:.2f}%)\")\n",
    "    print(f\"Normal records: {normal:,} ({normal/total*100:.2f}%)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Top anomalies with metrics\n",
    "    print(\"\\nüìä TOP 20 ANOMALIES (with deviation metrics)\")\n",
    "    top_anomalies = df[df['anomaly'] == -1].sort_values('avg_seats', ascending=False).head(20)\n",
    "    display(top_anomalies[['route', 'day_of_week', 'avg_seats', 'deviation_multiplier', 'std_deviations']])\n",
    "    \n",
    "    # Enriched anomalies with full names\n",
    "    print(\"\\nüåç TOP 20 ENRICHED ANOMALIES (with city/country names)\")\n",
    "    display(df_enriched.orderBy(\"avg_seats\", ascending=False).limit(20))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Execution Pipeline\n",
    "\n",
    "```python\n",
    "def run_anomaly_detection_pipeline():\n",
    "    \"\"\"\n",
    "    Execute the complete anomaly detection pipeline.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"üöÄ Starting Anomaly Detection Pipeline...\\n\")\n",
    "    \n",
    "    # Step 1: Load and prepare data\n",
    "    print(\"[1/5] Loading and preparing data...\")\n",
    "    df = load_and_prepare_anomaly_data()\n",
    "    \n",
    "    # Step 2: Invoke model\n",
    "    print(\"\\n[2/5] Invoking model endpoint...\")\n",
    "    df = invoke_anomaly_model(df)\n",
    "    \n",
    "    # Step 3: Calculate metrics\n",
    "    print(\"\\n[3/5] Calculating deviation metrics...\")\n",
    "    df = calculate_deviation_metrics(df)\n",
    "    \n",
    "    # Step 4: Enrich with IATA\n",
    "    print(\"\\n[4/5] Enriching with IATA data...\")\n",
    "    df_enriched = enrich_with_iata_data(df)\n",
    "    \n",
    "    # Step 5: Display results\n",
    "    print(\"\\n[5/5] Displaying results...\")\n",
    "    display_anomaly_results(df, df_enriched)\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    \n",
    "    return df, df_enriched\n",
    "\n",
    "# Execute pipeline\n",
    "df_results, df_enriched_results = run_anomaly_detection_pipeline()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Usage for Angular Application\n",
    "\n",
    "### API Integration Points\n",
    "\n",
    "1. **Data Endpoint**: Query `mc.amadeus2.anomaly_updates` via Databricks SQL API\n",
    "2. **Model Endpoint**: `https://<workspace-url>/serving-endpoints/flight-seat-anomaly-detector/invocations`\n",
    "3. **Results Format**: JSON with fields:\n",
    "   - `route`: Origin_to_Destination format\n",
    "   - `origin_city_full`: Full origin city name\n",
    "   - `origin_country_full`: Full origin country\n",
    "   - `destination_city_full`: Full destination city name\n",
    "   - `destination_country_full`: Full destination country\n",
    "   - `avg_seats`: Average seat count\n",
    "   - `deviation_multiplier`: Deviation from normal (e.g., \"+11x\")\n",
    "   - `std_deviations`: Standard deviations from mean\n",
    "\n",
    "### Expected Output Schema\n",
    "```json\n",
    "{\n",
    "  \"route\": \"CPH_to_DXB\",\n",
    "  \"origin_city\": \"CPH\",\n",
    "  \"origin_city_full\": \"Copenhagen - Copenhagen Airport\",\n",
    "  \"origin_country_full\": \"Denmark\",\n",
    "  \"destination_city\": \"DXB\",\n",
    "  \"destination_city_full\": \"Dubai - Dubai International Airport\",\n",
    "  \"destination_country_full\": \"United Arab Emirates\",\n",
    "  \"day_of_week\": 5,\n",
    "  \"avg_seats\": 1568,\n",
    "  \"deviation_multiplier\": \"+11x\",\n",
    "  \"std_deviations\": 29.39\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- **Normal Baseline**: Median of normal records (~127 seats)\n",
    "- **Anomaly Threshold**: Automatically determined by Isolation Forest model\n",
    "- **Deviation Multiplier**: Shows how many times above/below normal baseline\n",
    "- **Standard Deviations**: Statistical distance from mean of normal records\n",
    "- **Day of Week**: 1=Sunday, 2=Monday, ..., 7=Saturday (Spark dayofweek function)\n",
    "\n",
    "---\n",
    "\n",
    "## Error Handling\n",
    "\n",
    "- Handle NaN values in `avg_seats` by dropping rows\n",
    "- Verify model endpoint is running before calling\n",
    "- Check for missing IATA codes in lookup table (will result in NULL city/country)\n",
    "- Validate input data has required columns before processing"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "anomaly detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

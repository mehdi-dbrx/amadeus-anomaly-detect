{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811cd527-c1d7-4464-9e1c-3b6a8a865f68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Load data and select columns"
    }
   },
   "outputs": [],
   "source": [
    "# Load the table and select only required columns\n",
    "df = spark.table(\"mc.amadeus2.data_jan26\").select(\n",
    "    \"trip_origin_city\",\n",
    "    \"trip_destination_city\",\n",
    "    \"flight_leg_departure_date\",\n",
    "    \"flight_leg_total_seats\"\n",
    ")\n",
    "\n",
    "# Display sample data\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412a15a0-d628-48b4-a997-02b0ef0e4dd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Create route feature"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, dayofweek\n",
    "\n",
    "# Create route feature: origin_to_destination\n",
    "df_with_route = df.withColumn(\n",
    "    \"route\",\n",
    "    concat(\"trip_origin_city\", lit(\"_to_\"), \"trip_destination_city\")\n",
    ")\n",
    "\n",
    "# Extract day_of_week for grouping (1=Sunday, 7=Saturday)\n",
    "df_with_route = df_with_route.withColumn(\n",
    "    \"day_of_week\",\n",
    "    dayofweek(\"flight_leg_departure_date\")\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_with_route.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095790e6-03a3-49b7-acc8-337588e719cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Group by route and day_of_week, aggregate mean seats"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "# Group by route and day_of_week, calculate mean of flight_leg_total_seats\n",
    "df_aggregated = df_with_route.groupBy(\"route\", \"day_of_week\").agg(\n",
    "    round(avg(\"flight_leg_total_seats\"), 0).alias(\"avg_seats\")\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_aggregated.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9d1aae-b02d-424c-9568-d6ad5966e38f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: Convert to pandas and apply IsolationForest"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_pandas = df_aggregated.toPandas()\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "# Fit on avg_seats only (reshape for sklearn)\n",
    "X = df_pandas[['avg_seats']]\n",
    "iso_forest.fit(X)\n",
    "\n",
    "print(f\"Model trained on {len(df_pandas)} route-day combinations\")\n",
    "print(f\"avg_seats range: {df_pandas['avg_seats'].min()} - {df_pandas['avg_seats'].max()}\")\n",
    "display(df_pandas.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9b89f5-2dbe-4109-8dac-8794b3a3c187",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 6: Predict anomalies (-1 = anomaly, 1 = normal)"
    }
   },
   "outputs": [],
   "source": [
    "# Predict anomalies: -1 = anomaly, 1 = normal\n",
    "df_pandas['anomaly'] = iso_forest.predict(X)\n",
    "\n",
    "# Count anomalies\n",
    "anomalies_count = (df_pandas['anomaly'] == -1).sum()\n",
    "normal_count = (df_pandas['anomaly'] == 1).sum()\n",
    "\n",
    "print(f\"Total records: {len(df_pandas)}\")\n",
    "print(f\"Anomalies detected: {anomalies_count} ({anomalies_count/len(df_pandas)*100:.2f}%)\")\n",
    "print(f\"Normal records: {normal_count} ({normal_count/len(df_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Display sample with anomalies\n",
    "print(\"\\nSample of detected anomalies:\")\n",
    "display(df_pandas[df_pandas['anomaly'] == -1].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b30c99-042f-42b1-b90a-db76ee063a31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 7: Log model with MLflow"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"flight_seat_anomaly_detection\") as run:\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"IsolationForest\")\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"feature\", \"avg_seats\")\n",
    "    mlflow.log_param(\"total_records\", len(df_pandas))\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"anomalies_count\", anomalies_count)\n",
    "    mlflow.log_metric(\"anomalies_percentage\", anomalies_count/len(df_pandas)*100)\n",
    "    mlflow.log_metric(\"normal_count\", normal_count)\n",
    "    mlflow.log_metric(\"avg_seats_min\", df_pandas['avg_seats'].min())\n",
    "    mlflow.log_metric(\"avg_seats_max\", df_pandas['avg_seats'].max())\n",
    "    \n",
    "    # Create model signature\n",
    "    predictions = iso_forest.predict(X)\n",
    "    signature = infer_signature(X, predictions)\n",
    "    \n",
    "    # Log the model with signature\n",
    "    mlflow.sklearn.log_model(iso_forest, \"isolation_forest_model\", signature=signature)\n",
    "    \n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "    print(f\"Model logged successfully with signature!\")\n",
    "    print(f\"Experiment ID: {run.info.experiment_id}\")\n",
    "    print(f\"Artifact URI: {run.info.artifact_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579b72fa-3aa3-4f0f-92cc-32dc9d7760b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 8: Register model to Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "# Register the model to Unity Catalog\n",
    "model_name = \"mc.amadeus2.flight_seat_anomaly_detector\"\n",
    "\n",
    "# Get the model URI from the last run\n",
    "model_uri = f\"runs:/{run.info.run_id}/isolation_forest_model\"\n",
    "\n",
    "# Register the model\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model registered successfully!\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Model version: {registered_model.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464fbb1a-fdcf-4a4a-a22a-e9541709ed0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploy to Model Serving"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ServedEntityInput, EndpointCoreConfigInput\n",
    "\n",
    "# Initialize Databricks client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Define endpoint configuration\n",
    "endpoint_name = \"flight-seat-anomaly-detector\"\n",
    "model_name = \"mc.amadeus2.flight_seat_anomaly_detector\"\n",
    "model_version = registered_model.version\n",
    "\n",
    "# Create or update the serving endpoint\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=EndpointCoreConfigInput(\n",
    "            served_entities=[\n",
    "                ServedEntityInput(\n",
    "                    entity_name=model_name,\n",
    "                    entity_version=str(model_version),\n",
    "                    scale_to_zero_enabled=True,\n",
    "                    workload_size=\"Small\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    print(f\"Model serving endpoint created successfully!\")\n",
    "    print(f\"Endpoint name: {endpoint_name}\")\n",
    "    print(f\"Endpoint state: {endpoint.state.config_update}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"Endpoint '{endpoint_name}' already exists. Updating...\")\n",
    "        endpoint = w.serving_endpoints.update_config_and_wait(\n",
    "            name=endpoint_name,\n",
    "            served_entities=[\n",
    "                ServedEntityInput(\n",
    "                    entity_name=model_name,\n",
    "                    entity_version=str(model_version),\n",
    "                    scale_to_zero_enabled=True,\n",
    "                    workload_size=\"Small\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Endpoint updated successfully!\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99a90b4-d5f5-4930-b4e0-79bb3e4a6802",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load and prepare anomaly_updates data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, dayofweek, avg, round\n",
    "\n",
    "# Load the anomaly_updates table\n",
    "df_updates = spark.table(\"mc.amadeus2.anomaly_updates\").select(\n",
    "    \"trip_origin_city\",\n",
    "    \"trip_destination_city\",\n",
    "    \"flight_leg_departure_date\",\n",
    "    \"new_flight_leg_total_seats\"\n",
    ")\n",
    "\n",
    "# Create route feature: origin_to_destination\n",
    "df_updates_with_route = df_updates.withColumn(\n",
    "    \"route\",\n",
    "    concat(\"trip_origin_city\", lit(\"_to_\"), \"trip_destination_city\")\n",
    ")\n",
    "\n",
    "# Extract day_of_week for grouping\n",
    "df_updates_with_route = df_updates_with_route.withColumn(\n",
    "    \"day_of_week\",\n",
    "    dayofweek(\"flight_leg_departure_date\")\n",
    ")\n",
    "\n",
    "# Group by route and day_of_week, calculate mean of new_flight_leg_total_seats\n",
    "df_updates_aggregated = df_updates_with_route.groupBy(\"route\", \"day_of_week\").agg(\n",
    "    round(avg(\"new_flight_leg_total_seats\"), 0).alias(\"avg_seats\")\n",
    ")\n",
    "\n",
    "print(f\"Total route-day combinations to score: {df_updates_aggregated.count()}\")\n",
    "display(df_updates_aggregated.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "957cc258-dd68-4fe2-afad-6accfc912cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_updates_aggregated.write.mode(\"overwrite\").saveAsTable(\"mc.amadeus2.anomaly_updates_aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6519f3-63e0-4447-9903-88fa8d040407",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call deployed model endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get the endpoint URL and token\n",
    "endpoint_name = \"flight-seat-anomaly-detector\"\n",
    "token = w.dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "host = w.dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "\n",
    "# Prepare data for inference - convert to pandas and format as required by the model\n",
    "df_updates_pandas = df_updates_aggregated.toPandas()\n",
    "\n",
    "# Handle NaN values - drop rows with NaN in avg_seats\n",
    "print(f\"Total records before cleaning: {len(df_updates_pandas)}\")\n",
    "df_updates_pandas = df_updates_pandas.dropna(subset=['avg_seats'])\n",
    "print(f\"Total records after removing NaN: {len(df_updates_pandas)}\")\n",
    "\n",
    "# The model expects a dataframe with 'avg_seats' column\n",
    "input_data = df_updates_pandas[['avg_seats']].to_dict(orient='split')\n",
    "\n",
    "# Call the endpoint\n",
    "url = f\"{host}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"dataframe_split\": input_data\n",
    "}\n",
    "\n",
    "print(f\"\\nCalling endpoint: {endpoint_name}\")\n",
    "print(f\"Scoring {len(df_updates_pandas)} records...\")\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    predictions = response.json()['predictions']\n",
    "    df_updates_pandas['anomaly'] = predictions\n",
    "    print(f\"\\n✓ Successfully scored {len(predictions)} records\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    raise Exception(f\"Model endpoint call failed with status {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e47cfe-9843-4c3f-a40a-fac4be67ef3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyze and display anomaly results"
    }
   },
   "outputs": [],
   "source": [
    "# Count anomalies\n",
    "anomalies_count = (df_updates_pandas['anomaly'] == -1).sum()\n",
    "normal_count = (df_updates_pandas['anomaly'] == 1).sum()\n",
    "\n",
    "print(f\"\\n=== ANOMALY DETECTION RESULTS ===\")\n",
    "print(f\"Total records scored: {len(df_updates_pandas)}\")\n",
    "print(f\"Anomalies detected: {anomalies_count} ({anomalies_count/len(df_updates_pandas)*100:.2f}%)\")\n",
    "print(f\"Normal records: {normal_count} ({normal_count/len(df_updates_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Display anomalies sorted by avg_seats (highest first)\n",
    "print(\"\\n=== DETECTED ANOMALIES (sorted by avg_seats) ===\")\n",
    "anomalies_df = df_updates_pandas[df_updates_pandas['anomaly'] == -1].sort_values('avg_seats', ascending=False)\n",
    "display(anomalies_df)\n",
    "\n",
    "# Display normal records sample\n",
    "print(\"\\n=== SAMPLE OF NORMAL RECORDS ===\")\n",
    "normal_df = df_updates_pandas[df_updates_pandas['anomaly'] == 1].sort_values('avg_seats', ascending=False)\n",
    "display(normal_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71d18b4-8a40-4c09-bb4a-c9f5cb700914",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate anomaly scores and % deviation from normal"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load the model from Unity Catalog to get anomaly scores (use version 1)\n",
    "model_name = \"mc.amadeus2.flight_seat_anomaly_detector\"\n",
    "loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/1\")\n",
    "\n",
    "# Get anomaly scores from the model (more negative = more anomalous)\n",
    "anomalies_scores = loaded_model.score_samples(df_updates_pandas[['avg_seats']])\n",
    "df_updates_pandas['anomaly_score'] = anomalies_scores\n",
    "\n",
    "# Calculate baseline from normal records\n",
    "normal_baseline = df_updates_pandas[df_updates_pandas['anomaly'] == 1]['avg_seats'].median()\n",
    "normal_mean = df_updates_pandas[df_updates_pandas['anomaly'] == 1]['avg_seats'].mean()\n",
    "normal_std = df_updates_pandas[df_updates_pandas['anomaly'] == 1]['avg_seats'].std()\n",
    "\n",
    "# Calculate deviation multiplier (divide by 100, round to 0, add +/- sign)\n",
    "deviation_raw = ((df_updates_pandas['avg_seats'] - normal_baseline) / normal_baseline).round(0)\n",
    "df_updates_pandas['deviation_multiplier'] = deviation_raw.apply(lambda x: f\"+{int(x)}%\" if x > 0 else f\"{int(x)}x\")\n",
    "\n",
    "# Calculate standard deviations from mean\n",
    "df_updates_pandas['std_deviations'] = ((df_updates_pandas['avg_seats'] - normal_mean) / normal_std).round(2)\n",
    "\n",
    "print(f\"=== NORMAL BASELINE STATISTICS ===\")\n",
    "print(f\"Median (baseline): {normal_baseline:.0f} seats\")\n",
    "print(f\"Mean: {normal_mean:.2f} seats\")\n",
    "print(f\"Std Dev: {normal_std:.2f} seats\")\n",
    "print(f\"\\n=== TOP 20 ANOMALIES WITH DEVIATION METRICS ===\")\n",
    "\n",
    "# Show top anomalies with all metrics (hide anomaly_score)\n",
    "top_anomalies = df_updates_pandas[df_updates_pandas['anomaly'] == -1].sort_values('avg_seats', ascending=False).head(20)\n",
    "display(top_anomalies[['route', 'day_of_week', 'avg_seats', 'deviation_multiplier', 'std_deviations']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5a26de-f9da-4096-8813-1fde7edd349f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enrich anomalies with IATA city/country names"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Convert anomalies to Spark DataFrame\n",
    "df_anomalies_spark = spark.createDataFrame(df_updates_pandas[df_updates_pandas['anomaly'] == -1])\n",
    "\n",
    "# Split route to get origin and destination IATA codes\n",
    "df_anomalies_split = df_anomalies_spark.withColumn(\n",
    "    \"origin_city\", split(\"route\", \"_to_\")[0]\n",
    ").withColumn(\n",
    "    \"destination_city\", split(\"route\", \"_to_\")[1]\n",
    ")\n",
    "\n",
    "# Load IATA lookup table\n",
    "df_iata = spark.table(\"mc.amadeus2.iata\")\n",
    "\n",
    "# Join for origin city/country\n",
    "df_enriched = df_anomalies_split.alias(\"a\").join(\n",
    "    df_iata.alias(\"i_origin\"),\n",
    "    col(\"a.origin_city\") == col(\"i_origin.iata\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"a.*\"),\n",
    "    col(\"i_origin.city\").alias(\"origin_city_full\"),\n",
    "    col(\"i_origin.country\").alias(\"origin_country_full\")\n",
    ")\n",
    "\n",
    "# Join for destination city/country\n",
    "df_enriched = df_enriched.alias(\"a\").join(\n",
    "    df_iata.alias(\"i_dest\"),\n",
    "    col(\"a.destination_city\") == col(\"i_dest.iata\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"a.route\"),\n",
    "    col(\"a.origin_city\"),\n",
    "    col(\"a.origin_city_full\"),\n",
    "    col(\"a.origin_country_full\"),\n",
    "    col(\"a.destination_city\"),\n",
    "    col(\"i_dest.city\").alias(\"destination_city_full\"),\n",
    "    col(\"i_dest.country\").alias(\"destination_country_full\"),\n",
    "    col(\"a.day_of_week\"),\n",
    "    col(\"a.avg_seats\"),\n",
    "    col(\"a.deviation_multiplier\"),\n",
    "    col(\"a.std_deviations\")\n",
    ")\n",
    "\n",
    "print(f\"✓ Enriched {df_enriched.count()} anomalies with city/country names\")\n",
    "print(\"\\n=== Top 20 Enriched Anomalies ===\")\n",
    "display(df_enriched.orderBy(\"avg_seats\", ascending=False).limit(20))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "anomaly detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811cd527-c1d7-4464-9e1c-3b6a8a865f68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Load data and select columns"
    }
   },
   "outputs": [],
   "source": [
    "# Load the table and select only required columns\n",
    "df = spark.table(\"mc.amadeus2.data_jan26\").select(\n",
    "    \"trip_origin_city\",\n",
    "    \"trip_destination_city\",\n",
    "    \"flight_leg_departure_date\",\n",
    "    \"flight_leg_total_seats\"\n",
    ")\n",
    "\n",
    "# Display sample data\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412a15a0-d628-48b4-a997-02b0ef0e4dd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Create route feature"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, dayofweek\n",
    "\n",
    "# Create route feature: origin_to_destination\n",
    "df_with_route = df.withColumn(\n",
    "    \"route\",\n",
    "    concat(\"trip_origin_city\", lit(\"_to_\"), \"trip_destination_city\")\n",
    ")\n",
    "\n",
    "# Extract day_of_week for grouping (1=Sunday, 7=Saturday)\n",
    "df_with_route = df_with_route.withColumn(\n",
    "    \"day_of_week\",\n",
    "    dayofweek(\"flight_leg_departure_date\")\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_with_route.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095790e6-03a3-49b7-acc8-337588e719cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Group by route and day_of_week, aggregate mean seats"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "# Group by route and day_of_week, calculate mean of flight_leg_total_seats\n",
    "df_aggregated = df_with_route.groupBy(\"route\", \"day_of_week\").agg(\n",
    "    round(avg(\"flight_leg_total_seats\"), 0).alias(\"avg_seats\")\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_aggregated.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9d1aae-b02d-424c-9568-d6ad5966e38f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: Convert to pandas and apply IsolationForest"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_pandas = df_aggregated.toPandas()\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "# Fit on avg_seats only (reshape for sklearn)\n",
    "X = df_pandas[['avg_seats']]\n",
    "iso_forest.fit(X)\n",
    "\n",
    "print(f\"Model trained on {len(df_pandas)} route-day combinations\")\n",
    "print(f\"avg_seats range: {df_pandas['avg_seats'].min()} - {df_pandas['avg_seats'].max()}\")\n",
    "display(df_pandas.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9b89f5-2dbe-4109-8dac-8794b3a3c187",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 6: Predict anomalies (-1 = anomaly, 1 = normal)"
    }
   },
   "outputs": [],
   "source": [
    "# Predict anomalies: -1 = anomaly, 1 = normal\n",
    "df_pandas['anomaly'] = iso_forest.predict(X)\n",
    "\n",
    "# Count anomalies\n",
    "anomalies_count = (df_pandas['anomaly'] == -1).sum()\n",
    "normal_count = (df_pandas['anomaly'] == 1).sum()\n",
    "\n",
    "print(f\"Total records: {len(df_pandas)}\")\n",
    "print(f\"Anomalies detected: {anomalies_count} ({anomalies_count/len(df_pandas)*100:.2f}%)\")\n",
    "print(f\"Normal records: {normal_count} ({normal_count/len(df_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Display sample with anomalies\n",
    "print(\"\\nSample of detected anomalies:\")\n",
    "display(df_pandas[df_pandas['anomaly'] == -1].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b30c99-042f-42b1-b90a-db76ee063a31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 7: Log model with MLflow"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"flight_seat_anomaly_detection\") as run:\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"IsolationForest\")\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"feature\", \"avg_seats\")\n",
    "    mlflow.log_param(\"total_records\", len(df_pandas))\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"anomalies_count\", anomalies_count)\n",
    "    mlflow.log_metric(\"anomalies_percentage\", anomalies_count/len(df_pandas)*100)\n",
    "    mlflow.log_metric(\"normal_count\", normal_count)\n",
    "    mlflow.log_metric(\"avg_seats_min\", df_pandas['avg_seats'].min())\n",
    "    mlflow.log_metric(\"avg_seats_max\", df_pandas['avg_seats'].max())\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(iso_forest, \"isolation_forest_model\")\n",
    "    \n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "    print(f\"Model logged successfully!\")\n",
    "    print(f\"Experiment ID: {run.info.experiment_id}\")\n",
    "    print(f\"Artifact URI: {run.info.artifact_uri}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "anomaly detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
